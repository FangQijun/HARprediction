<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>Machine Learning Method for HAR Prediction Based on Movement Data</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: rgb(88, 72, 246)
   }

   pre .number {
     color: rgb(0, 0, 205);
   }

   pre .comment {
     color: rgb(76, 136, 107);
   }

   pre .keyword {
     color: rgb(0, 0, 255);
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: rgb(3, 106, 7);
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>


<!-- MathJax scripts -->
<script type="text/javascript" src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



</head>

<body>
<h1>Machine Learning Method for HAR Prediction Based on Movement Data</h1>

<p>\[ Author: Q.F.Fang \]   </p>

<h2>Synopsis</h2>

<p>According to the project <a href="https://class.coursera.org/predmachlearn-002/human_grading/view/courses/972090/assessments/4/submissions">background</a>, devices such as Jawbone Up, Nike FuelBand, and Fitbit now allow us to collect a large amount of data about human activity recognition (HAR) in a more economical fashion. With such data, we are allowed to predict the quality of human&#39;s movement.</p>

<p>We utilized the <a href="http://groupware.les.inf.puc-rio.br/har">training and testing dataset</a> from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. We built a Machine Learning model on it, in order to predict the manner in which they performed in the experiment (i.e. the &ldquo;classe&rdquo; variable in the training set). Generally, what we did include:   </p>

<hr/>

<ol>
<li>Load the data and filter out irrelavant variables.</li>
<li>Use the <strong>PCA Method</strong> to select out variables making a somewhat significant contribution to the outcome variable, &ldquo;classe&rdquo;.</li>
<li>Split the training data into two parts, one for training and the other for cross-validation.</li>
<li>Train the training set using <strong>&ldquo;K-fold Algorithm&rdquo; + &ldquo;Random Forest Algorithm&rdquo;</strong> and verify its feasibility by calculating the &ldquo;out of sample error&rdquo; on cross-validation data.</li>
<li>Pass the machine learning method on the testing set to make prediction.</li>
</ol>

<h2>Step 1: Loading &amp; Cleaning Raw Data</h2>

<p>First things first, we load all the packages that might be of use in our project, set the working directory and read the training and testing .csv datasets into &ldquo;tr&rdquo; and &ldquo;te&rdquo; objects respectively. Note that all blank and NA observations as read as NA&#39;s.</p>

<pre><code class="r">library(caret)
</code></pre>

<pre><code>## Loading required package: lattice
## Loading required package: ggplot2
</code></pre>

<pre><code class="r">library(lattice)
library(ggplot2)
library(randomForest)
</code></pre>

<pre><code>## randomForest 4.6-7
## Type rfNews() to see new features/changes/bug fixes.
</code></pre>

<pre><code class="r">setwd(&quot;D:\\Learning Materials\\Coursera_Practical Machine Learning\\Project&quot;)
tr &lt;- read.csv(&quot;.\\pml-training.csv&quot;, na.strings = c(&quot;NA&quot;, &quot;&quot;))
te &lt;- read.csv(&quot;.\\pml-testing.csv&quot;, na.strings = c(&quot;NA&quot;, &quot;&quot;))  ##all blank and NA observations as read as NA&#39;s
str(tr)
</code></pre>

<p>By checking the structure of &ldquo;tr&rdquo;, we find out that, of all 160 variables, the first 7 ones are merely recording labels of each observation instead of describing the activities. Hence they need filtering out.</p>

<pre><code class="r">tr &lt;- tr[, -c(1:7)]
</code></pre>

<p>Besides, many variables consist of a considerable proportion of blank observation values. We then only kept variables without any NAs in the &ldquo;tr&rdquo; dataset. The consequent datasets are named &ldquo;training&rdquo;.</p>

<pre><code class="r">bad &lt;- rep(NA, ncol(tr))  ## Create a null vector &#39;bad&#39; to record variables with NAs.
for (i in 1:ncol(tr)) {
    if (any(is.na(tr[, i]))) 
        {
            bad[i] &lt;- i
        }  ## As long as a variable contains NA, it is labelled as &#39;bad&#39; variable.
}
bad &lt;- bad[!is.na(bad)]
training &lt;- tr[, -bad]  ## Throw away &#39;tr&#39; columns labelled as &#39;bad&#39; and assign the remaining data to &#39;training&#39;
</code></pre>

<h2>Step 2: Selecting Variables of Significant Contribution</h2>

<p>Now we check the dimension of the &ldquo;training&rdquo; dataset.</p>

<pre><code class="r">dim(training)
</code></pre>

<pre><code>## [1] 19622    53
</code></pre>

<p>It turns out we reduce the number of variables from 160 to 53, including the outcome &ldquo;classe&rdquo;. We may regard the remaining 52 as potential regressors. However, we still believe there too many regressors. Some insignificant ones ought to be excluded from our model.<br/>
Correlations among all variables are measures of variable significance. Thereby, we use the PCA Algorithm to filter out variables with correlation values below a certain threshold. Setting the threshold level is quite crucial: if the threshold is too low, insignificant variables might fail to be excluded; if the threshold is too high, there might be too few regressors to predict the outcome so that the prediction accuracy can be negatively affected. After multiple trials, we set the threshold level at 0.6.   </p>

<pre><code class="r">M &lt;- abs(cor(training[, -53]))  ## M is the correlation matrix of all 52 potential regressors.
diag(M) &lt;- 0  ## Obviously, cor(XX,XX)=1. We ignore such self-correlation values by let the diagonal values be 0.
A &lt;- which(M &gt; 0.6, arr.ind = T)  ## Keep matrix entries above the threshold level, 0.6
sigVars &lt;- unique(c(A[, 1], A[, 2]))  ## With unique() function, we get the &#39;sigVars&#39; vector consisting of column numbers correspondant to signicant variables.
length(unique(c(A[, 1], A[, 2])))
</code></pre>

<pre><code>## [1] 40
</code></pre>

<p>Till now, we reduce the number of potential regressors from 52 to 40 and we obtain the new training set, &ldquo;training&rdquo;:</p>

<pre><code class="r">training &lt;- cbind(training[, sigVars], tr$classe)
colnames(training)[ncol(training)] &lt;- &quot;classe&quot;
</code></pre>

<h2>Step 3: Spliting &ldquo;training&rdquo; Dataset for Cross-Validation</h2>

<p>Within the &ldquo;caret&rdquo; R-package, we split the training data into two parts, one for training (i.e., &ldquo;realTraining&rdquo;) and the other for cross-validation (i.e., &ldquo;Xvalidation&rdquo;).</p>

<pre><code class="r">set.seed(10011)  ## Set randomization seed for reproduciblility
inTrain &lt;- createDataPartition(y = training$classe, p = 0.2, list = FALSE)  ## The training set accounts for 20% of the &#39;training&#39; dataser.
realTraining &lt;- training[inTrain, ]
Xvalidation &lt;- tr[-inTrain, ]
dim(Xvalidation)
</code></pre>

<pre><code>## [1] 15695   153
</code></pre>

<pre><code class="r">dim(realTraining)
</code></pre>

<pre><code>## [1] 3927   41
</code></pre>

<h2>Step 4: Training &amp; Cross-Validation</h2>

<p>Train the training set using <strong>&ldquo;K-fold Algorithm&rdquo; + &ldquo;Random Forest Algorithm&rdquo;</strong> and verify its feasibility by calculating the &ldquo;out of sample error&rdquo; on cross-validation data. Hereby, K=4. Specially, to expediate the training process, we fix some settings of the &ldquo;trControl&rdquo; call. We expect a somehow acceptable out of sample error less than 10%.</p>

<pre><code class="r">set.seed(10086)
modFit &lt;- train(classe ~ ., data = realTraining, method = &quot;rf&quot;, trControl = trainControl(method = &quot;cv&quot;, 
    number = 4))
modFit
</code></pre>

<pre><code>## Random Forest 
## 
## 3927 samples
##   40 predictors
##    5 classes: &#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (4 fold) 
## 
## Summary of sample sizes: 2945, 2945, 2946, 2945 
## 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy  Kappa  Accuracy SD  Kappa SD
##   2     0.9       0.9    0.007        0.009   
##   20    0.9       0.9    0.005        0.007   
##   40    0.9       0.9    0.005        0.006   
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 21.
</code></pre>

<p><strong>Note that the &ldquo;Accuracy&rdquo; values here in the outcome are truncated</strong>. In fact, however, it should be &ldquo;Accuracy=0.95&rdquo;. Hence:<br/>
\[ in-sample-error=1- max(Accuracy)= \approx 1-0.95=5.0\% \]
The in sample error is quite low.<br/>
Then, we  pass the &ldquo;modFit&rdquo; to &ldquo;Xvalidation&rdquo; for cross-validation:</p>

<pre><code class="r">Xprediction &lt;- predict(modFit, Xvalidation)
Xtable &lt;- table(Xprediction, Xvalidation$classe)
Xtable
</code></pre>

<pre><code>##            
## Xprediction    A    B    C    D    E
##           A 4418   63   10   46   12
##           B   14 2889   58    9   20
##           C    4   81 2651  114   29
##           D   23    4   18 2398   16
##           E    5    0    0    5 2808
</code></pre>

<p>See the table above, we declaim that:<br/>
\[ out-of-sample-error(of cross-validation)=1-\frac{sum(diag(Xtable))}{ncol(Xvalidation)}=1-\frac{4418+2889+2651+2398+2808}{15695} \approx 3.39\% \]<br/>
The out of sample error is even lower. Therefore we have reasons to believe in the feasibility of the machine learning model.</p>

<h2>Step 5: Final Prediction</h2>

<p>Finally, we pass the machine learning method, &ldquo;modFit&rdquo;, to the testing set, &ldquo;te&rdquo;, to make prediction.</p>

<pre><code class="r">pred &lt;- predict(modFit, te)
pred
</code></pre>

<pre><code>##  [1] B A A A A E D B A A B C B A E E A B B B
## Levels: A B C D E
</code></pre>

<p>&ldquo;pred&rdquo; is the 20 prediction results. However, according to <a href="https://class.coursera.org/predmachlearn-002/assignment">the course project auto-grader submission system</a>, the third outcome of &ldquo;pred&rdquo;, &ldquo;A&rdquo;,  is incorrect actually.   </p>

<p>Hence,<br/>
\[ out-of-sample-error(of testing set) =\frac{19}{20}= 5\% \]</p>

<h2>Summary &amp; Declaration</h2>

<p>The machine learning prediction model in this work is rather reliable and convenient.   </p>

<p>The out of sample errors may result from insufficient number of regressors and K value in the K-fold Algorithm. Also, the out of sample errors vary among different seed we set.</p>

</body>

</html>

